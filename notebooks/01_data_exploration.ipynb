{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5eed2c0",
   "metadata": {},
   "source": [
    "# TAEG (Temporal Alignment Event Graph) - Data Exploration\n",
    "\n",
    "This notebook implements a complete exploratory data analysis for the TAEG project for abstractive multi-document summarization using Graph Neural Networks (GNNs) and Natural Language Processing (NLP).\n",
    "\n",
    "## Objective\n",
    "Explore and analyze the five XML files that form the basis of the TAEG system:\n",
    "- ChronologyOfTheFourGospels_PW.xml (event structure)\n",
    "- EnglishNIVMatthew40_PW.xml, EnglishNIVMark41_PW.xml, EnglishNIVLuke42_PW.xml, EnglishNIVJohn43_PW.xml (gospel texts)\n",
    "\n",
    "## Notebook Structure\n",
    "1. **Data Loading and XML Parsing** - Loading and parsing XMLs\n",
    "2. **Graph Construction from Chronology** - Temporal graph construction \n",
    "3. **Node Content Extraction and Alignment** - Node content extraction\n",
    "4. **Temporal Edge Creation** - Temporal edge creation\n",
    "5. **TAEG Model Implementation** - TAEG model implementation\n",
    "6. **Baseline Models Setup** - Baseline models configuration\n",
    "7. **Training Pipeline** - Training pipeline\n",
    "8. **Evaluation and Metrics** - Evaluation and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c798df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÅ Project directory: {project_root}\")\n",
    "print(f\"üìÅ Data directory: {project_root / 'data'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91172276",
   "metadata": {},
   "source": [
    "## 2. Data Loading and XML Parsing\n",
    "\n",
    "Loading and parsing the XML files that contain the chronology and gospel texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TAEG modules\n",
    "try:\n",
    "    from data_loader import DataLoader\n",
    "    from graph_builder import TAEGGraphBuilder\n",
    "    print(\"‚úÖ TAEG modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing TAEG modules: {e}\")\n",
    "    print(\"‚ö†Ô∏è Creating simple data loader for exploration...\")\n",
    "\n",
    "class SimpleDataLoader:\n",
    "    \"\"\"Simple data loader for XML file exploration\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.gospel_files = {\n",
    "            'matthew': 'EnglishNIVMatthew40_PW.xml',\n",
    "            'mark': 'EnglishNIVMark41_PW.xml',\n",
    "            'luke': 'EnglishNIVLuke42_PW.xml',\n",
    "            'john': 'EnglishNIVJohn43_PW.xml'\n",
    "        }\n",
    "        self.chronology_file = 'ChronologyOfTheFourGospels_PW.xml'\n",
    "    \n",
    "    def check_files(self):\n",
    "        \"\"\"Check if all files exist\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Check chronology file\n",
    "        chronology_path = self.data_dir / self.chronology_file\n",
    "        results['chronology'] = {\n",
    "            'exists': chronology_path.exists(),\n",
    "            'path': chronology_path,\n",
    "            'size': chronology_path.stat().st_size if chronology_path.exists() else 0\n",
    "        }\n",
    "        \n",
    "        # Check gospel files\n",
    "        results['gospels'] = {}\n",
    "        for gospel, filename in self.gospel_files.items():\n",
    "            filepath = self.data_dir / filename\n",
    "            results['gospels'][gospel] = {\n",
    "                'exists': filepath.exists(),\n",
    "                'path': filepath,\n",
    "                'size': filepath.stat().st_size if filepath.exists() else 0\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize data loader\n",
    "data_dir = project_root / \"data\"\n",
    "loader = SimpleDataLoader(data_dir)\n",
    "\n",
    "# Check files\n",
    "file_status = loader.check_files()\n",
    "\n",
    "print(\"üìä Data files status:\")\n",
    "print(f\"Chronology: {'‚úÖ' if file_status['chronology']['exists'] else '‚ùå'} ({file_status['chronology']['size']:,} bytes)\")\n",
    "\n",
    "for gospel, info in file_status['gospels'].items():\n",
    "    status = '‚úÖ' if info['exists'] else '‚ùå'\n",
    "    size = f\"({info['size']:,} bytes)\" if info['exists'] else \"(file not found)\"\n",
    "    print(f\"{gospel.title()}: {status} {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para analisar a estrutura de um arquivo XML\n",
    "def analyze_xml_structure(xml_path, max_depth=3, sample_size=5):\n",
    "    \"\"\"Analisa a estrutura de um arquivo XML\"\"\"\n",
    "    if not xml_path.exists():\n",
    "        return {\"error\": \"Arquivo n√£o encontrado\"}\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        def get_element_info(element, depth=0):\n",
    "            info = {\n",
    "                'tag': element.tag,\n",
    "                'attributes': dict(element.attrib),\n",
    "                'text_length': len(element.text.strip()) if element.text else 0,\n",
    "                'children_count': len(list(element)),\n",
    "                'depth': depth\n",
    "            }\n",
    "            \n",
    "            if depth < max_depth and len(list(element)) > 0:\n",
    "                children = list(element)[:sample_size]  # Amostra dos primeiros filhos\n",
    "                info['children_sample'] = [get_element_info(child, depth + 1) for child in children]\n",
    "            \n",
    "            return info\n",
    "        \n",
    "        structure = get_element_info(root)\n",
    "        \n",
    "        # Estat√≠sticas gerais\n",
    "        all_elements = list(root.iter())\n",
    "        tags_count = Counter(elem.tag for elem in all_elements)\n",
    "        \n",
    "        return {\n",
    "            'root_info': structure,\n",
    "            'total_elements': len(all_elements),\n",
    "            'unique_tags': len(tags_count),\n",
    "            'tag_frequencies': dict(tags_count.most_common(10)),\n",
    "            'file_size': xml_path.stat().st_size\n",
    "        }\n",
    "        \n",
    "    except ET.ParseError as e:\n",
    "        return {\"error\": f\"Erro de parsing XML: {e}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Erro inesperado: {e}\"}\n",
    "\n",
    "# Analisar estrutura do arquivo de cronologia\n",
    "print(\"üîç Analisando estrutura do arquivo de cronologia...\")\n",
    "chronology_analysis = analyze_xml_structure(file_status['chronology']['path'])\n",
    "\n",
    "if 'error' not in chronology_analysis:\n",
    "    print(f\"üìä Elementos totais: {chronology_analysis['total_elements']}\")\n",
    "    print(f\"üè∑Ô∏è Tags √∫nicas: {chronology_analysis['unique_tags']}\")\n",
    "    print(f\"üìà Tags mais frequentes: {list(chronology_analysis['tag_frequencies'].keys())[:5]}\")\n",
    "    print(f\"üîñ Tag raiz: {chronology_analysis['root_info']['tag']}\")\n",
    "    print(f\"üë∂ Filhos da raiz: {chronology_analysis['root_info']['children_count']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Erro: {chronology_analysis['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0386662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar estrutura dos arquivos dos evangelhos\n",
    "print(\"üîç Analisando estrutura dos arquivos dos evangelhos...\")\n",
    "\n",
    "gospel_analyses = {}\n",
    "for gospel, info in file_status['gospels'].items():\n",
    "    if info['exists']:\n",
    "        print(f\"\\nüìñ Analisando {gospel.title()}...\")\n",
    "        analysis = analyze_xml_structure(info['path'])\n",
    "        gospel_analyses[gospel] = analysis\n",
    "        \n",
    "        if 'error' not in analysis:\n",
    "            print(f\"   üìä Elementos: {analysis['total_elements']}\")\n",
    "            print(f\"   üè∑Ô∏è Tags √∫nicas: {analysis['unique_tags']}\")\n",
    "            print(f\"   üìà Tags principais: {list(analysis['tag_frequencies'].keys())[:3]}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {analysis['error']}\")\n",
    "\n",
    "# Criar DataFrame para compara√ß√£o\n",
    "if gospel_analyses:\n",
    "    comparison_data = []\n",
    "    for gospel, analysis in gospel_analyses.items():\n",
    "        if 'error' not in analysis:\n",
    "            comparison_data.append({\n",
    "                'Evangelho': gospel.title(),\n",
    "                'Elementos_Totais': analysis['total_elements'],\n",
    "                'Tags_Unicas': analysis['unique_tags'],\n",
    "                'Tamanho_Arquivo_KB': analysis['file_size'] / 1024,\n",
    "                'Tag_Raiz': analysis['root_info']['tag']\n",
    "            })\n",
    "    \n",
    "    if comparison_data:\n",
    "        df_gospels = pd.DataFrame(comparison_data)\n",
    "        print(\"\\nüìä Compara√ß√£o dos evangelhos:\")\n",
    "        print(df_gospels.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1172fd42",
   "metadata": {},
   "source": [
    "## 3. Graph Construction from Chronology\n",
    "\n",
    "Building the temporal event graph based on the chronology XML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77603818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para extrair eventos da cronologia\n",
    "def extract_chronology_events(xml_path):\n",
    "    \"\"\"Extrai eventos do arquivo de cronologia\"\"\"\n",
    "    if not xml_path.exists():\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        events = []\n",
    "        for event_elem in root.findall('.//event'):\n",
    "            event_id = event_elem.get('id', '')\n",
    "            if not event_id:\n",
    "                continue\n",
    "            \n",
    "            # Extrair refer√™ncias para cada evangelho\n",
    "            event_data = {'id': event_id}\n",
    "            gospels = ['matthew', 'mark', 'luke', 'john']\n",
    "            \n",
    "            for gospel in gospels:\n",
    "                gospel_elem = event_elem.find(gospel)\n",
    "                if gospel_elem is not None and gospel_elem.text:\n",
    "                    event_data[gospel] = gospel_elem.text.strip()\n",
    "                else:\n",
    "                    event_data[gospel] = None\n",
    "            \n",
    "            events.append(event_data)\n",
    "        \n",
    "        return events\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair eventos: {e}\")\n",
    "        return []\n",
    "\n",
    "# Extrair eventos da cronologia\n",
    "print(\"üìÖ Extraindo eventos da cronologia...\")\n",
    "events = extract_chronology_events(file_status['chronology']['path'])\n",
    "\n",
    "print(f\"üìä Total de eventos extra√≠dos: {len(events)}\")\n",
    "\n",
    "if events:\n",
    "    # Analisar cobertura por evangelho\n",
    "    gospel_coverage = {}\n",
    "    for gospel in ['matthew', 'mark', 'luke', 'john']:\n",
    "        count = sum(1 for event in events if event.get(gospel) is not None)\n",
    "        gospel_coverage[gospel] = count\n",
    "        print(f\"   {gospel.title()}: {count} eventos ({count/len(events)*100:.1f}%)\")\n",
    "    \n",
    "    # Mostrar alguns exemplos\n",
    "    print(f\"\\nüîç Primeiros 3 eventos como exemplo:\")\n",
    "    for i, event in enumerate(events[:3]):\n",
    "        print(f\"\\nEvento {event['id']}:\")\n",
    "        for gospel in ['matthew', 'mark', 'luke', 'john']:\n",
    "            ref = event.get(gospel, 'N/A')\n",
    "            print(f\"   {gospel.title()}: {ref}\")\n",
    "    \n",
    "    # Criar DataFrame para an√°lise\n",
    "    events_df = pd.DataFrame(events)\n",
    "    print(f\"\\nüìä Dimens√µes do DataFrame: {events_df.shape}\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum evento foi extra√≠do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abe969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir grafo b√°sico usando NetworkX\n",
    "def build_basic_graph(events):\n",
    "    \"\"\"Constr√≥i um grafo b√°sico a partir dos eventos\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Adicionar n√≥s (eventos)\n",
    "    for event in events:\n",
    "        event_id = event['id']\n",
    "        participating_gospels = [g for g in ['matthew', 'mark', 'luke', 'john'] \n",
    "                               if event.get(g) is not None]\n",
    "        \n",
    "        G.add_node(event_id, \n",
    "                   participating_gospels=participating_gospels,\n",
    "                   num_gospels=len(participating_gospels))\n",
    "    \n",
    "    # Adicionar arestas temporais para cada evangelho\n",
    "    gospels = ['matthew', 'mark', 'luke', 'john']\n",
    "    edge_count = 0\n",
    "    \n",
    "    for gospel in gospels:\n",
    "        # Obter sequ√™ncia de eventos para este evangelho\n",
    "        gospel_events = [event['id'] for event in events if event.get(gospel) is not None]\n",
    "        \n",
    "        # Criar arestas sequenciais\n",
    "        for i in range(len(gospel_events) - 1):\n",
    "            current_event = gospel_events[i]\n",
    "            next_event = gospel_events[i + 1]\n",
    "            \n",
    "            if G.has_edge(current_event, next_event):\n",
    "                # Aresta j√° existe, adicionar este evangelho\n",
    "                G[current_event][next_event]['gospels'].append(gospel)\n",
    "                G[current_event][next_event]['weight'] += 1\n",
    "            else:\n",
    "                # Criar nova aresta\n",
    "                G.add_edge(current_event, next_event, \n",
    "                          gospels=[gospel], weight=1)\n",
    "                edge_count += 1\n",
    "    \n",
    "    return G, edge_count\n",
    "\n",
    "if events:\n",
    "    print(\"üï∏Ô∏è Construindo grafo temporal...\")\n",
    "    graph, unique_edges = build_basic_graph(events)\n",
    "    \n",
    "    print(f\"üìä Estat√≠sticas do grafo:\")\n",
    "    print(f\"   N√≥s (eventos): {graph.number_of_nodes()}\")\n",
    "    print(f\"   Arestas √∫nicas: {unique_edges}\")\n",
    "    print(f\"   Arestas totais: {graph.number_of_edges()}\")\n",
    "    print(f\"   Componentes conectados: {nx.number_weakly_connected_components(graph)}\")\n",
    "    \n",
    "    # Analisar graus dos n√≥s\n",
    "    degrees = [d for n, d in graph.degree()]\n",
    "    in_degrees = [d for n, d in graph.in_degree()]\n",
    "    out_degrees = [d for n, d in graph.out_degree()]\n",
    "    \n",
    "    print(f\"   Grau m√©dio: {np.mean(degrees):.2f}\")\n",
    "    print(f\"   Grau m√°ximo: {max(degrees) if degrees else 0}\")\n",
    "    print(f\"   In-degree m√©dio: {np.mean(in_degrees):.2f}\")\n",
    "    print(f\"   Out-degree m√©dio: {np.mean(out_degrees):.2f}\")\n",
    "    \n",
    "    # N√≥s por n√∫mero de evangelhos\n",
    "    gospels_distribution = {}\n",
    "    for node in graph.nodes():\n",
    "        num_gospels = graph.nodes[node]['num_gospels']\n",
    "        gospels_distribution[num_gospels] = gospels_distribution.get(num_gospels, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìñ Distribui√ß√£o por n√∫mero de evangelhos:\")\n",
    "    for num_gospels, count in sorted(gospels_distribution.items()):\n",
    "        print(f\"   {num_gospels} evangelho(s): {count} eventos ({count/len(events)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec09bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o b√°sica do grafo\n",
    "if events and 'graph' in locals():\n",
    "    print(\"üìä Criando visualiza√ß√µes do grafo...\")\n",
    "    \n",
    "    # Configurar figura com subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('TAEG - An√°lise do Grafo Temporal de Eventos', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Distribui√ß√£o de graus\n",
    "    degrees = [d for n, d in graph.degree()]\n",
    "    axes[0, 0].hist(degrees, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribui√ß√£o de Graus dos N√≥s')\n",
    "    axes[0, 0].set_xlabel('Grau')\n",
    "    axes[0, 0].set_ylabel('Frequ√™ncia')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Participa√ß√£o por evangelho\n",
    "    gospel_counts = [gospel_coverage[g] for g in ['matthew', 'mark', 'luke', 'john']]\n",
    "    gospel_names = ['Mateus', 'Marcos', 'Lucas', 'Jo√£o']\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    \n",
    "    bars = axes[0, 1].bar(gospel_names, gospel_counts, color=colors, alpha=0.8)\n",
    "    axes[0, 1].set_title('Eventos por Evangelho')\n",
    "    axes[0, 1].set_ylabel('N√∫mero de Eventos')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar, count in zip(bars, gospel_counts):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                        str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Distribui√ß√£o por n√∫mero de evangelhos\n",
    "    num_gospels_data = list(gospels_distribution.keys())\n",
    "    num_gospels_counts = list(gospels_distribution.values())\n",
    "    \n",
    "    axes[1, 0].pie(num_gospels_counts, labels=[f'{n} Evang.' for n in num_gospels_data], \n",
    "                   autopct='%1.1f%%', startangle=90, colors=['#FFE66D', '#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    axes[1, 0].set_title('Eventos por N√∫mero de Evangelhos')\n",
    "    \n",
    "    # 4. Componentes conectados (se houver m√∫ltiplos)\n",
    "    components = list(nx.weakly_connected_components(graph))\n",
    "    component_sizes = [len(comp) for comp in components]\n",
    "    \n",
    "    if len(components) > 1:\n",
    "        axes[1, 1].bar(range(1, len(components) + 1), component_sizes, \n",
    "                       color='lightcoral', alpha=0.8)\n",
    "        axes[1, 1].set_title('Tamanho dos Componentes Conectados')\n",
    "        axes[1, 1].set_xlabel('Componente')\n",
    "        axes[1, 1].set_ylabel('N√∫mero de N√≥s')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, f'Grafo Conectado\\\\n{len(component_sizes[0])} n√≥s', \n",
    "                        ha='center', va='center', transform=axes[1, 1].transAxes,\n",
    "                        fontsize=14, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "        axes[1, 1].set_title('Conectividade do Grafo')\n",
    "        axes[1, 1].set_xticks([])\n",
    "        axes[1, 1].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualiza√ß√µes criadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9aefc9",
   "metadata": {},
   "source": [
    "## 4. Node Content Extraction and Alignment\n",
    "\n",
    "Extracting textual content for each event node from the four gospels and aligning them temporally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ba7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para parsear refer√™ncias de vers√≠culos\n",
    "import re\n",
    "\n",
    "def parse_verse_reference(ref_str):\n",
    "    \"\"\"Parse uma refer√™ncia de vers√≠culo (ex: '2:1-12' ou '1:5')\"\"\"\n",
    "    if not ref_str or ref_str.strip() == '':\n",
    "        return []\n",
    "    \n",
    "    # Padr√£o para capturar cap√≠tulo:verso ou cap√≠tulo:verso-verso\n",
    "    pattern = r'(\\d+):(\\d+)(?:-(\\d+))?'\n",
    "    matches = re.findall(pattern, ref_str.strip())\n",
    "    \n",
    "    references = []\n",
    "    for match in matches:\n",
    "        chapter = int(match[0])\n",
    "        start_verse = int(match[1])\n",
    "        end_verse = int(match[2]) if match[2] else start_verse\n",
    "        \n",
    "        references.append({\n",
    "            'chapter': chapter,\n",
    "            'start_verse': start_verse,\n",
    "            'end_verse': end_verse\n",
    "        })\n",
    "    \n",
    "    return references\n",
    "\n",
    "# Fun√ß√£o para extrair texto simplificado de um evangelho\n",
    "def extract_gospel_text_simple(xml_path, target_chapter, target_verses):\n",
    "    \"\"\"Extra√ß√£o simplificada de texto de evangelho\"\"\"\n",
    "    if not xml_path.exists():\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Esta √© uma implementa√ß√£o simplificada\n",
    "        # Na pr√°tica, voc√™ precisaria adaptar para a estrutura real dos XMLs\n",
    "        \n",
    "        # Procurar elementos que contenham o texto\n",
    "        texts = []\n",
    "        for elem in root.iter():\n",
    "            if elem.text and elem.text.strip():\n",
    "                # Simplificado: assumir que encontramos texto relevante\n",
    "                if len(elem.text.strip()) > 10:  # Filtrar textos muito curtos\n",
    "                    texts.append(elem.text.strip())\n",
    "        \n",
    "        # Retornar uma amostra do texto (limitado para demonstra√ß√£o)\n",
    "        if texts:\n",
    "            return \" \".join(texts[:3])  # Primeiros 3 textos encontrados\n",
    "        \n",
    "        return f\"[Texto para cap√≠tulo {target_chapter}, vers√≠culos {target_verses} n√£o encontrado]\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"[Erro ao extrair texto: {e}]\"\n",
    "\n",
    "# Analisar refer√™ncias de vers√≠culos nos eventos\n",
    "print(\"üìñ Analisando refer√™ncias de vers√≠culos...\")\n",
    "\n",
    "verse_analysis = {\n",
    "    'total_references': 0,\n",
    "    'by_gospel': {'matthew': 0, 'mark': 0, 'luke': 0, 'john': 0},\n",
    "    'reference_patterns': [],\n",
    "    'sample_extractions': []\n",
    "}\n",
    "\n",
    "if events:\n",
    "    for i, event in enumerate(events[:10]):  # Analisar primeiros 10 eventos\n",
    "        event_refs = {}\n",
    "        \n",
    "        for gospel in ['matthew', 'mark', 'luke', 'john']:\n",
    "            ref_str = event.get(gospel)\n",
    "            if ref_str:\n",
    "                refs = parse_verse_reference(ref_str)\n",
    "                event_refs[gospel] = refs\n",
    "                verse_analysis['by_gospel'][gospel] += len(refs)\n",
    "                verse_analysis['total_references'] += len(refs)\n",
    "                \n",
    "                # Coletar padr√µes de refer√™ncia\n",
    "                if refs:\n",
    "                    verse_analysis['reference_patterns'].append({\n",
    "                        'event_id': event['id'],\n",
    "                        'gospel': gospel,\n",
    "                        'reference': ref_str,\n",
    "                        'parsed_count': len(refs)\n",
    "                    })\n",
    "        \n",
    "        # Amostra de extra√ß√£o de texto (para demonstra√ß√£o)\n",
    "        if i < 3:  # Apenas para os primeiros 3 eventos\n",
    "            sample_text = {}\n",
    "            for gospel in ['matthew', 'mark', 'luke', 'john']:\n",
    "                if event.get(gospel) and gospel in file_status['gospels'] and file_status['gospels'][gospel]['exists']:\n",
    "                    refs = event_refs.get(gospel, [])\n",
    "                    if refs:\n",
    "                        ref = refs[0]  # Primeira refer√™ncia\n",
    "                        text = extract_gospel_text_simple(\n",
    "                            file_status['gospels'][gospel]['path'],\n",
    "                            ref['chapter'],\n",
    "                            range(ref['start_verse'], ref['end_verse'] + 1)\n",
    "                        )\n",
    "                        sample_text[gospel] = text[:200] + \"...\" if len(text) > 200 else text\n",
    "            \n",
    "            verse_analysis['sample_extractions'].append({\n",
    "                'event_id': event['id'],\n",
    "                'texts': sample_text\n",
    "            })\n",
    "\n",
    "print(f\"üìä Resultados da an√°lise:\")\n",
    "print(f\"   Total de refer√™ncias: {verse_analysis['total_references']}\")\n",
    "for gospel, count in verse_analysis['by_gospel'].items():\n",
    "    print(f\"   {gospel.title()}: {count} refer√™ncias\")\n",
    "\n",
    "print(f\"\\nüîç Padr√µes de refer√™ncia (primeiros 5):\")\n",
    "for pattern in verse_analysis['reference_patterns'][:5]:\n",
    "    print(f\"   Evento {pattern['event_id']} - {pattern['gospel']}: {pattern['reference']} ({pattern['parsed_count']} refs)\")\n",
    "\n",
    "print(f\"\\nüìù Amostras de texto extra√≠do:\")\n",
    "for sample in verse_analysis['sample_extractions']:\n",
    "    print(f\"\\n   Evento {sample['event_id']}:\")\n",
    "    for gospel, text in sample['texts'].items():\n",
    "        print(f\"      {gospel.title()}: {text[:100]}{'...' if len(text) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec9a1b",
   "metadata": {},
   "source": [
    "## 4. Temporal Edge Analysis\n",
    "\n",
    "Detailed analysis of temporal edges in the TAEG graph and their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ac064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada das arestas temporais\n",
    "def analyze_temporal_edges(graph, events):\n",
    "    \"\"\"Analisa as arestas temporais do grafo\"\"\"\n",
    "    analysis = {\n",
    "        'total_edges': graph.number_of_edges(),\n",
    "        'edges_by_gospel': {'matthew': 0, 'mark': 0, 'luke': 0, 'john': 0},\n",
    "        'edge_weights': [],\n",
    "        'multi_gospel_edges': 0,\n",
    "        'sequential_paths': {},\n",
    "        'edge_details': []\n",
    "    }\n",
    "    \n",
    "    # Analisar cada aresta\n",
    "    for source, target, data in graph.edges(data=True):\n",
    "        edge_gospels = data.get('gospels', [])\n",
    "        edge_weight = data.get('weight', 1)\n",
    "        \n",
    "        analysis['edge_weights'].append(edge_weight)\n",
    "        \n",
    "        # Contar arestas por evangelho\n",
    "        for gospel in edge_gospels:\n",
    "            analysis['edges_by_gospel'][gospel] += 1\n",
    "        \n",
    "        # Arestas com m√∫ltiplos evangelhos\n",
    "        if len(edge_gospels) > 1:\n",
    "            analysis['multi_gospel_edges'] += 1\n",
    "        \n",
    "        # Detalhes das primeiras arestas\n",
    "        if len(analysis['edge_details']) < 10:\n",
    "            analysis['edge_details'].append({\n",
    "                'source': source,\n",
    "                'target': target,\n",
    "                'gospels': edge_gospels,\n",
    "                'weight': edge_weight\n",
    "            })\n",
    "    \n",
    "    # Analisar caminhos sequenciais por evangelho\n",
    "    for gospel in ['matthew', 'mark', 'luke', 'john']:\n",
    "        gospel_events = [event['id'] for event in events if event.get(gospel) is not None]\n",
    "        analysis['sequential_paths'][gospel] = {\n",
    "            'length': len(gospel_events),\n",
    "            'first_event': gospel_events[0] if gospel_events else None,\n",
    "            'last_event': gospel_events[-1] if gospel_events else None,\n",
    "            'sample_sequence': gospel_events[:5] if len(gospel_events) >= 5 else gospel_events\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "if events and 'graph' in locals():\n",
    "    print(\"üîó Analisando arestas temporais...\")\n",
    "    edge_analysis = analyze_temporal_edges(graph, events)\n",
    "    \n",
    "    print(f\"üìä Estat√≠sticas das arestas:\")\n",
    "    print(f\"   Total de arestas: {edge_analysis['total_edges']}\")\n",
    "    print(f\"   Arestas com m√∫ltiplos evangelhos: {edge_analysis['multi_gospel_edges']}\")\n",
    "    print(f\"   Peso m√©dio das arestas: {np.mean(edge_analysis['edge_weights']):.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìñ Arestas por evangelho:\")\n",
    "    for gospel, count in edge_analysis['edges_by_gospel'].items():\n",
    "        print(f\"   {gospel.title()}: {count} arestas\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Caminhos sequenciais:\")\n",
    "    for gospel, path_info in edge_analysis['sequential_paths'].items():\n",
    "        print(f\"   {gospel.title()}: {path_info['length']} eventos\")\n",
    "        print(f\"      Primeiro: {path_info['first_event']} ‚Üí √öltimo: {path_info['last_event']}\")\n",
    "        print(f\"      Amostra: {' ‚Üí '.join(path_info['sample_sequence'])}\")\n",
    "    \n",
    "    print(f\"\\nüîç Detalhes das primeiras arestas:\")\n",
    "    for detail in edge_analysis['edge_details'][:5]:\n",
    "        gospels_str = ', '.join(detail['gospels'])\n",
    "        print(f\"   {detail['source']} ‚Üí {detail['target']} (peso: {detail['weight']}, evangelhos: {gospels_str})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o interativa do grafo com Plotly\n",
    "def create_interactive_graph_visualization(graph, max_nodes=50):\n",
    "    \"\"\"Cria visualiza√ß√£o interativa do grafo usando Plotly\"\"\"\n",
    "    \n",
    "    # Limitar n√∫mero de n√≥s para visualiza√ß√£o\n",
    "    if graph.number_of_nodes() > max_nodes:\n",
    "        # Pegar uma amostra dos n√≥s mais conectados\n",
    "        degree_centrality = nx.degree_centrality(graph)\n",
    "        top_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:max_nodes]\n",
    "        subgraph = graph.subgraph([node for node, _ in top_nodes])\n",
    "    else:\n",
    "        subgraph = graph\n",
    "    \n",
    "    # Calcular posi√ß√µes usando layout spring\n",
    "    pos = nx.spring_layout(subgraph, k=1, iterations=50)\n",
    "    \n",
    "    # Preparar dados dos n√≥s\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    node_colors = []\n",
    "    \n",
    "    for node in subgraph.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        \n",
    "        # Informa√ß√µes do n√≥\n",
    "        num_gospels = subgraph.nodes[node].get('num_gospels', 0)\n",
    "        participating_gospels = subgraph.nodes[node].get('participating_gospels', [])\n",
    "        \n",
    "        node_text.append(f\"Evento: {node}<br>Evangelhos: {num_gospels}<br>Detalhes: {', '.join(participating_gospels)}\")\n",
    "        \n",
    "        # Cor baseada no n√∫mero de evangelhos\n",
    "        if num_gospels == 1:\n",
    "            node_colors.append('lightblue')\n",
    "        elif num_gospels == 2:\n",
    "            node_colors.append('lightgreen')\n",
    "        elif num_gospels == 3:\n",
    "            node_colors.append('orange')\n",
    "        else:\n",
    "            node_colors.append('red')\n",
    "    \n",
    "    # Preparar dados das arestas\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_text = []\n",
    "    \n",
    "    for edge in subgraph.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "    \n",
    "    # Criar traces\n",
    "    edge_trace = go.Scatter(x=edge_x, y=edge_y,\n",
    "                           line=dict(width=0.5, color='#888'),\n",
    "                           hoverinfo='none',\n",
    "                           mode='lines')\n",
    "    \n",
    "    node_trace = go.Scatter(x=node_x, y=node_y,\n",
    "                           mode='markers',\n",
    "                           hoverinfo='text',\n",
    "                           text=node_text,\n",
    "                           marker=dict(size=10,\n",
    "                                      color=node_colors,\n",
    "                                      line=dict(width=2, color='black')))\n",
    "    \n",
    "    # Criar figura\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                   layout=go.Layout(\n",
    "                       title=f'TAEG - Grafo Temporal de Eventos (Amostra de {subgraph.number_of_nodes()} n√≥s)',\n",
    "                       titlefont_size=16,\n",
    "                       showlegend=False,\n",
    "                       hovermode='closest',\n",
    "                       margin=dict(b=20,l=5,r=5,t=40),\n",
    "                       annotations=[ dict(\n",
    "                           text=\"Cores: Azul=1 evangelho, Verde=2, Laranja=3, Vermelho=4\",\n",
    "                           showarrow=False,\n",
    "                           xref=\"paper\", yref=\"paper\",\n",
    "                           x=0.005, y=-0.002,\n",
    "                           xanchor=\"left\", yanchor=\"bottom\",\n",
    "                           font=dict(color=\"black\", size=12)\n",
    "                       )],\n",
    "                       xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                       yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "if events and 'graph' in locals():\n",
    "    print(\"üé® Criando visualiza√ß√£o interativa do grafo...\")\n",
    "    try:\n",
    "        interactive_fig = create_interactive_graph_visualization(graph)\n",
    "        interactive_fig.show()\n",
    "        print(\"‚úÖ Visualiza√ß√£o interativa criada! Passe o mouse sobre os n√≥s para ver detalhes.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao criar visualiza√ß√£o interativa: {e}\")\n",
    "        print(\"Continuando sem a visualiza√ß√£o interativa...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e4278",
   "metadata": {},
   "source": [
    "## 5. TAEG Model Implementation\n",
    "\n",
    "In this section, we will implement and test the TAEG (Temporal Alignment Event Graph) model that was developed for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ef296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar m√≥dulos do modelo TAEG\n",
    "sys.path.append('../src')\n",
    "from models import TAEGModel, GraphAttentionEncoder\n",
    "from graph_builder import TAEGGraphBuilder\n",
    "\n",
    "# Configura√ß√£o do modelo\n",
    "model_config = {\n",
    "    'input_dim': 768,  # Dimens√£o dos embeddings BERT\n",
    "    'hidden_dim': 256,\n",
    "    'num_attention_heads': 8,\n",
    "    'num_gat_layers': 2,\n",
    "    'dropout': 0.1,\n",
    "    'bart_model_name': 'facebook/bart-base'\n",
    "}\n",
    "\n",
    "print(\"üß† Configura√ß√£o do Modelo TAEG:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in model_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Fun√ß√£o para criar e inicializar o modelo\n",
    "def create_taeg_model(config):\n",
    "    \"\"\"Cria e inicializa o modelo TAEG\"\"\"\n",
    "    try:\n",
    "        model = TAEGModel(\n",
    "            input_dim=config['input_dim'],\n",
    "            hidden_dim=config['hidden_dim'],\n",
    "            num_attention_heads=config['num_attention_heads'],\n",
    "            num_gat_layers=config['num_gat_layers'],\n",
    "            dropout=config['dropout'],\n",
    "            bart_model_name=config['bart_model_name']\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao criar modelo: {e}\")\n",
    "        return None\n",
    "\n",
    "# Criar o modelo\n",
    "print(\"\\nüîß Criando modelo TAEG...\")\n",
    "try:\n",
    "    taeg_model = create_taeg_model(model_config)\n",
    "    if taeg_model:\n",
    "        print(\"‚úÖ Modelo TAEG criado com sucesso!\")\n",
    "        \n",
    "        # Informa√ß√µes sobre o modelo\n",
    "        total_params = sum(p.numel() for p in taeg_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in taeg_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"üìä Total de par√¢metros: {total_params:,}\")\n",
    "        print(f\"üìä Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "        \n",
    "        # Estrutura do modelo\n",
    "        print(\"\\nüèóÔ∏è Estrutura do modelo:\")\n",
    "        print(taeg_model)\n",
    "    else:\n",
    "        print(\"‚ùå Falha ao criar o modelo\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro durante cria√ß√£o do modelo: {e}\")\n",
    "    taeg_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af09381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de infer√™ncia com dados de exemplo\n",
    "def test_model_inference(model, graph, sample_text=\"Jesus ensinou aos disc√≠pulos.\"):\n",
    "    \"\"\"Testa infer√™ncia do modelo com dados de exemplo\"\"\"\n",
    "    \n",
    "    if model is None or graph is None:\n",
    "        print(\"‚ùå Modelo ou grafo n√£o dispon√≠vel para teste\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Converter grafo para formato PyTorch Geometric\n",
    "        builder = TAEGGraphBuilder()\n",
    "        torch_data = builder._convert_to_torch_geometric(graph)\n",
    "        \n",
    "        print(f\"üìä Dados do grafo para infer√™ncia:\")\n",
    "        print(f\"   - N√∫mero de n√≥s: {torch_data.x.shape[0]}\")\n",
    "        print(f\"   - Dimens√£o dos features: {torch_data.x.shape[1]}\")\n",
    "        print(f\"   - N√∫mero de arestas: {torch_data.edge_index.shape[1]}\")\n",
    "        \n",
    "        # Preparar input para o modelo\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Simular embeddings de entrada (normalmente viriam do BERT)\n",
    "            if torch_data.x.shape[1] != model_config['input_dim']:\n",
    "                # Ajustar dimens√£o se necess√°rio\n",
    "                input_features = torch.randn(torch_data.x.shape[0], model_config['input_dim'])\n",
    "            else:\n",
    "                input_features = torch_data.x\n",
    "            \n",
    "            print(f\"\\nüîÑ Executando infer√™ncia com texto: '{sample_text}'\")\n",
    "            \n",
    "            # Forward pass atrav√©s do encoder GAT\n",
    "            gat_output = model.gat_encoder(input_features, torch_data.edge_index)\n",
    "            print(f\"‚úÖ Sa√≠da do encoder GAT: {gat_output.shape}\")\n",
    "            \n",
    "            # Simular gera√ß√£o de resumo\n",
    "            sample_summary = model.generate_summary([sample_text], max_length=50)\n",
    "            print(f\"üìù Resumo gerado: {sample_summary[0] if sample_summary else 'Erro na gera√ß√£o'}\")\n",
    "            \n",
    "            return gat_output, sample_summary\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante teste de infer√™ncia: {e}\")\n",
    "        return None\n",
    "\n",
    "# Executar teste de infer√™ncia\n",
    "if 'taeg_model' in locals() and taeg_model and 'graph' in locals() and graph:\n",
    "    print(\"üß™ Testando infer√™ncia do modelo...\")\n",
    "    inference_result = test_model_inference(taeg_model, graph)\n",
    "    \n",
    "    if inference_result:\n",
    "        print(\"‚úÖ Teste de infer√™ncia conclu√≠do com sucesso!\")\n",
    "    else:\n",
    "        print(\"‚ùå Teste de infer√™ncia falhou\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Modelo ou grafo n√£o dispon√≠vel para teste de infer√™ncia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a0d6c1",
   "metadata": {},
   "source": [
    "## 6. Baseline Models Configuration\n",
    "\n",
    "To compare TAEG performance, we will configure the baseline models: PEGASUS, PRIMERA and LexRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar modelos baseline\n",
    "from models import PegasusBaseline, PrimeraBaseline, LexRankBaseline\n",
    "\n",
    "# Configura√ß√µes dos modelos baseline\n",
    "baseline_configs = {\n",
    "    'pegasus': {\n",
    "        'model_name': 'google/pegasus-xsum',\n",
    "        'max_length': 128,\n",
    "        'min_length': 30\n",
    "    },\n",
    "    'primera': {\n",
    "        'model_name': 'allenai/PRIMERA',\n",
    "        'max_length': 256,\n",
    "        'min_length': 50\n",
    "    },\n",
    "    'lexrank': {\n",
    "        'sentence_count': 5,\n",
    "        'threshold': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Configura√ß√µes dos Modelos Baseline:\")\n",
    "print(\"=\" * 60)\n",
    "for model_name, config in baseline_configs.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Fun√ß√£o para inicializar modelos baseline\n",
    "def initialize_baseline_models():\n",
    "    \"\"\"Inicializa todos os modelos baseline\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüîß Inicializando PEGASUS...\")\n",
    "        models['pegasus'] = PegasusBaseline(\n",
    "            model_name=baseline_configs['pegasus']['model_name']\n",
    "        )\n",
    "        print(\"‚úÖ PEGASUS inicializado\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao inicializar PEGASUS: {e}\")\n",
    "        models['pegasus'] = None\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüîß Inicializando PRIMERA...\")\n",
    "        models['primera'] = PrimeraBaseline(\n",
    "            model_name=baseline_configs['primera']['model_name']\n",
    "        )\n",
    "        print(\"‚úÖ PRIMERA inicializado\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao inicializar PRIMERA: {e}\")\n",
    "        models['primera'] = None\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüîß Inicializando LexRank...\")\n",
    "        models['lexrank'] = LexRankBaseline()\n",
    "        print(\"‚úÖ LexRank inicializado\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao inicializar LexRank: {e}\")\n",
    "        models['lexrank'] = None\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Inicializar modelos baseline\n",
    "print(\"üöÄ Inicializando modelos baseline...\")\n",
    "baseline_models = initialize_baseline_models()\n",
    "\n",
    "# Verificar quais modelos foram inicializados com sucesso\n",
    "available_models = [name for name, model in baseline_models.items() if model is not None]\n",
    "failed_models = [name for name, model in baseline_models.items() if model is None]\n",
    "\n",
    "print(f\"\\nüìä Resumo da inicializa√ß√£o:\")\n",
    "print(f\"‚úÖ Modelos dispon√≠veis: {', '.join(available_models) if available_models else 'Nenhum'}\")\n",
    "print(f\"‚ùå Modelos com falha: {', '.join(failed_models) if failed_models else 'Nenhum'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70120319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste r√°pido dos modelos baseline\n",
    "def test_baseline_models(models, sample_texts):\n",
    "    \"\"\"Testa rapidamente os modelos baseline com textos de exemplo\"\"\"\n",
    "    \n",
    "    if not sample_texts:\n",
    "        sample_texts = [\n",
    "            \"Jesus ensinou aos disc√≠pulos sobre o Reino de Deus.\",\n",
    "            \"Os ap√≥stolos seguiram Jesus durante sua jornada.\",\n",
    "            \"A mensagem de amor e perd√£o foi central nos ensinamentos.\"\n",
    "        ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        if model is None:\n",
    "            print(f\"‚ö†Ô∏è {model_name.upper()} n√£o dispon√≠vel para teste\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"\\nüß™ Testando {model_name.upper()}...\")\n",
    "            \n",
    "            if model_name == 'lexrank':\n",
    "                # LexRank precisa de m√∫ltiplos documentos\n",
    "                summary = model.summarize(sample_texts)\n",
    "            else:\n",
    "                # PEGASUS e PRIMERA podem trabalhar com texto concatenado\n",
    "                combined_text = \" \".join(sample_texts)\n",
    "                summary = model.summarize([combined_text])\n",
    "                summary = summary[0] if summary else \"Erro na gera√ß√£o\"\n",
    "            \n",
    "            results[model_name] = summary\n",
    "            print(f\"üìù Resumo gerado: {summary[:100]}{'...' if len(summary) > 100 else ''}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao testar {model_name}: {e}\")\n",
    "            results[model_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Preparar textos de exemplo dos evangelhos\n",
    "sample_gospel_texts = []\n",
    "if events:\n",
    "    # Extrair alguns textos dos eventos carregados\n",
    "    sample_events = list(events.keys())[:3]\n",
    "    for event_name in sample_events:\n",
    "        verses = data_loader.get_verses_for_event(event_name)\n",
    "        if verses:\n",
    "            sample_texts = [verse.text for verse in verses[:2]]  # Primeiros 2 versos\n",
    "            sample_gospel_texts.extend(sample_texts)\n",
    "\n",
    "if not sample_gospel_texts:\n",
    "    sample_gospel_texts = [\n",
    "        \"E Jesus, vendo as multid√µes, subiu ao monte; e, assentando-se, aproximaram-se dele os seus disc√≠pulos.\",\n",
    "        \"E, abrindo a sua boca, os ensinava, dizendo: Bem-aventurados os pobres de esp√≠rito, porque deles √© o reino dos c√©us.\",\n",
    "        \"Bem-aventurados os que choram, porque eles ser√£o consolados.\"\n",
    "    ]\n",
    "\n",
    "print(\"üìö Textos de exemplo para teste:\")\n",
    "for i, text in enumerate(sample_gospel_texts[:3], 1):\n",
    "    print(f\"{i}. {text[:80]}{'...' if len(text) > 80 else ''}\")\n",
    "\n",
    "# Executar testes dos modelos baseline\n",
    "if baseline_models:\n",
    "    print(\"\\nüß™ Executando testes dos modelos baseline...\")\n",
    "    baseline_results = test_baseline_models(baseline_models, sample_gospel_texts)\n",
    "    \n",
    "    print(f\"\\nüìä Resumo dos resultados:\")\n",
    "    for model_name, result in baseline_results.items():\n",
    "        status = \"‚úÖ Sucesso\" if result else \"‚ùå Falha\"\n",
    "        print(f\"{model_name.upper()}: {status}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum modelo baseline dispon√≠vel para teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31bf48a",
   "metadata": {},
   "source": [
    "## 7. Training Pipeline\n",
    "\n",
    "Configuration and testing of the training pipeline for the TAEG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e37102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar m√≥dulos de treinamento\n",
    "from train import TAEGTrainer, TrainingConfig, EarlyStopping\n",
    "\n",
    "# Configura√ß√£o de treinamento\n",
    "training_config = TrainingConfig(\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_epochs=10,\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "    early_stopping_patience=3,\n",
    "    save_every_n_epochs=2\n",
    ")\n",
    "\n",
    "print(\"üéØ Configura√ß√£o de Treinamento:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Batch Size: {training_config.batch_size}\")\n",
    "print(f\"Learning Rate: {training_config.learning_rate}\")\n",
    "print(f\"N√∫mero de √âpocas: {training_config.num_epochs}\")\n",
    "print(f\"Warmup Steps: {training_config.warmup_steps}\")\n",
    "print(f\"Max Grad Norm: {training_config.max_grad_norm}\")\n",
    "print(f\"Early Stopping Patience: {training_config.early_stopping_patience}\")\n",
    "print(f\"Salvar a cada N √©pocas: {training_config.save_every_n_epochs}\")\n",
    "\n",
    "# Fun√ß√£o para preparar dados de treinamento\n",
    "def prepare_training_data(events, data_loader, train_ratio=0.8):\n",
    "    \"\"\"Prepara dados para treinamento dividindo em train/val\"\"\"\n",
    "    \n",
    "    if not events:\n",
    "        print(\"‚ùå Nenhum evento dispon√≠vel para treinamento\")\n",
    "        return None, None\n",
    "    \n",
    "    # Converter eventos em pares (input, target)\n",
    "    training_pairs = []\n",
    "    \n",
    "    for event_name, event_data in events.items():\n",
    "        verses = data_loader.get_verses_for_event(event_name)\n",
    "        if verses and len(verses) >= 2:\n",
    "            # Usar versos como input e criar um resumo simples como target\n",
    "            input_texts = [verse.text for verse in verses]\n",
    "            # Target simplificado (para demonstra√ß√£o)\n",
    "            target_summary = f\"Resumo do evento {event_name}: {verses[0].text[:50]}...\"\n",
    "            \n",
    "            training_pairs.append({\n",
    "                'input_texts': input_texts,\n",
    "                'target_summary': target_summary,\n",
    "                'event_name': event_name\n",
    "            })\n",
    "    \n",
    "    if not training_pairs:\n",
    "        print(\"‚ùå Nenhum par de treinamento criado\")\n",
    "        return None, None\n",
    "    \n",
    "    # Dividir em treino e valida√ß√£o\n",
    "    split_idx = int(len(training_pairs) * train_ratio)\n",
    "    train_data = training_pairs[:split_idx]\n",
    "    val_data = training_pairs[split_idx:]\n",
    "    \n",
    "    print(f\"üìä Dados preparados:\")\n",
    "    print(f\"   Total de pares: {len(training_pairs)}\")\n",
    "    print(f\"   Treinamento: {len(train_data)}\")\n",
    "    print(f\"   Valida√ß√£o: {len(val_data)}\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# Preparar dados de treinamento\n",
    "if events and data_loader:\n",
    "    print(\"\\nüì¶ Preparando dados de treinamento...\")\n",
    "    train_data, val_data = prepare_training_data(events, data_loader)\n",
    "    \n",
    "    if train_data and val_data:\n",
    "        print(\"‚úÖ Dados de treinamento preparados com sucesso!\")\n",
    "        \n",
    "        # Mostrar exemplo de dados\n",
    "        print(f\"\\nüìã Exemplo de dado de treinamento:\")\n",
    "        example = train_data[0]\n",
    "        print(f\"Evento: {example['event_name']}\")\n",
    "        print(f\"Textos de entrada: {len(example['input_texts'])} versos\")\n",
    "        print(f\"Primeiro verso: {example['input_texts'][0][:100]}...\")\n",
    "        print(f\"Resumo alvo: {example['target_summary']}\")\n",
    "    else:\n",
    "        print(\"‚ùå Falha ao preparar dados de treinamento\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Eventos ou data_loader n√£o dispon√≠veis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1603c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar trainer e executar treinamento de demonstra√ß√£o\n",
    "def setup_trainer(model, graph, config):\n",
    "    \"\"\"Configura o trainer para o modelo TAEG\"\"\"\n",
    "    \n",
    "    if model is None or graph is None:\n",
    "        print(\"‚ùå Modelo ou grafo n√£o dispon√≠vel\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        trainer = TAEGTrainer(\n",
    "            model=model,\n",
    "            config=config,\n",
    "            device='cpu'  # Usar CPU para demonstra√ß√£o\n",
    "        )\n",
    "        \n",
    "        # Converter grafo para formato PyTorch Geometric\n",
    "        builder = TAEGGraphBuilder()\n",
    "        torch_data = builder._convert_to_torch_geometric(graph)\n",
    "        \n",
    "        print(f\"‚úÖ Trainer configurado com sucesso\")\n",
    "        print(f\"üìä Dados do grafo: {torch_data.x.shape[0]} n√≥s, {torch_data.edge_index.shape[1]} arestas\")\n",
    "        \n",
    "        return trainer, torch_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao configurar trainer: {e}\")\n",
    "        return None\n",
    "\n",
    "# Simular uma √©poca de treinamento\n",
    "def simulate_training_epoch(trainer, torch_data, train_data):\n",
    "    \"\"\"Simula uma √©poca de treinamento (apenas para demonstra√ß√£o)\"\"\"\n",
    "    \n",
    "    if not trainer or not torch_data or not train_data:\n",
    "        print(\"‚ùå Componentes necess√°rios n√£o dispon√≠veis\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ Simulando √©poca de treinamento...\")\n",
    "        \n",
    "        # Em um treinamento real, isso seria feito com batches\n",
    "        sample_batch = train_data[:min(2, len(train_data))]  # Pequeno batch para demo\n",
    "        \n",
    "        # Simular m√©tricas de treinamento\n",
    "        simulated_metrics = {\n",
    "            'loss': 2.5 + np.random.normal(0, 0.1),\n",
    "            'learning_rate': training_config.learning_rate,\n",
    "            'grad_norm': np.random.uniform(0.5, 1.5)\n",
    "        }\n",
    "        \n",
    "        print(f\"üìä M√©tricas simuladas:\")\n",
    "        for metric, value in simulated_metrics.items():\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "        \n",
    "        return simulated_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante simula√ß√£o: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configurar e testar o trainer\n",
    "if 'taeg_model' in locals() and taeg_model and 'graph' in locals() and graph:\n",
    "    print(\"üîß Configurando trainer...\")\n",
    "    trainer_setup = setup_trainer(taeg_model, graph, training_config)\n",
    "    \n",
    "    if trainer_setup and 'train_data' in locals() and train_data:\n",
    "        trainer, torch_data = trainer_setup\n",
    "        \n",
    "        print(\"\\nüß™ Executando simula√ß√£o de treinamento...\")\n",
    "        training_metrics = simulate_training_epoch(trainer, torch_data, train_data)\n",
    "        \n",
    "        if training_metrics:\n",
    "            print(\"‚úÖ Simula√ß√£o de treinamento conclu√≠da!\")\n",
    "            \n",
    "            # Simular hist√≥rico de treinamento\n",
    "            print(f\"\\nüìà Hist√≥rico simulado de 3 √©pocas:\")\n",
    "            for epoch in range(1, 4):\n",
    "                loss = 2.5 - (epoch * 0.3) + np.random.normal(0, 0.1)\n",
    "                print(f\"√âpoca {epoch}: Loss = {loss:.4f}\")\n",
    "        else:\n",
    "            print(\"‚ùå Falha na simula√ß√£o de treinamento\")\n",
    "    else:\n",
    "        print(\"‚ùå Falha ao configurar trainer ou dados n√£o dispon√≠veis\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Modelo TAEG ou grafo n√£o dispon√≠veis para teste de treinamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fc18f",
   "metadata": {},
   "source": [
    "## 8. Evaluation and Metrics\n",
    "\n",
    "Configuration and testing of the evaluation system with ROUGE, BERTScore and temporal coherence metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar m√≥dulos de avalia√ß√£o\n",
    "from evaluate import ROUGEEvaluator, BERTScoreEvaluator, TemporalCoherenceEvaluator\n",
    "\n",
    "# Configurar avaliadores\n",
    "print(\"üéØ Configurando avaliadores...\")\n",
    "\n",
    "evaluators = {}\n",
    "\n",
    "try:\n",
    "    print(\"üîß Inicializando ROUGE Evaluator...\")\n",
    "    evaluators['rouge'] = ROUGEEvaluator()\n",
    "    print(\"‚úÖ ROUGE Evaluator inicializado\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao inicializar ROUGE: {e}\")\n",
    "    evaluators['rouge'] = None\n",
    "\n",
    "try:\n",
    "    print(\"üîß Inicializando BERTScore Evaluator...\")\n",
    "    evaluators['bertscore'] = BERTScoreEvaluator()\n",
    "    print(\"‚úÖ BERTScore Evaluator inicializado\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao inicializar BERTScore: {e}\")\n",
    "    evaluators['bertscore'] = None\n",
    "\n",
    "try:\n",
    "    print(\"üîß Inicializando Temporal Coherence Evaluator...\")\n",
    "    evaluators['temporal'] = TemporalCoherenceEvaluator()\n",
    "    print(\"‚úÖ Temporal Coherence Evaluator inicializado\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao inicializar Temporal Coherence: {e}\")\n",
    "    evaluators['temporal'] = None\n",
    "\n",
    "# Verificar avaliadores dispon√≠veis\n",
    "available_evaluators = [name for name, eval in evaluators.items() if eval is not None]\n",
    "print(f\"\\nüìä Avaliadores dispon√≠veis: {', '.join(available_evaluators) if available_evaluators else 'Nenhum'}\")\n",
    "\n",
    "# Fun√ß√£o para testar avalia√ß√£o\n",
    "def test_evaluation_metrics(evaluators, predictions, references):\n",
    "    \"\"\"Testa m√©tricas de avalia√ß√£o com dados de exemplo\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for eval_name, evaluator in evaluators.items():\n",
    "        if evaluator is None:\n",
    "            print(f\"‚ö†Ô∏è {eval_name.upper()} n√£o dispon√≠vel\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüß™ Testando {eval_name.upper()}...\")\n",
    "            \n",
    "            if eval_name == 'rouge':\n",
    "                scores = evaluator.compute_rouge(predictions, references)\n",
    "                results[eval_name] = scores\n",
    "                print(f\"üìä ROUGE-1: {scores.get('rouge1', 'N/A')}\")\n",
    "                print(f\"üìä ROUGE-2: {scores.get('rouge2', 'N/A')}\")\n",
    "                print(f\"üìä ROUGE-L: {scores.get('rougeL', 'N/A')}\")\n",
    "                \n",
    "            elif eval_name == 'bertscore':\n",
    "                scores = evaluator.compute_bertscore(predictions, references)\n",
    "                results[eval_name] = scores\n",
    "                if scores:\n",
    "                    print(f\"üìä BERTScore F1: {np.mean(scores.get('f1', [0])):.4f}\")\n",
    "                    print(f\"üìä BERTScore Precision: {np.mean(scores.get('precision', [0])):.4f}\")\n",
    "                    print(f\"üìä BERTScore Recall: {np.mean(scores.get('recall', [0])):.4f}\")\n",
    "                \n",
    "            elif eval_name == 'temporal':\n",
    "                # Para coer√™ncia temporal, precisamos de dados temporais\n",
    "                if 'graph' in locals() and graph:\n",
    "                    scores = evaluator.evaluate_temporal_coherence(predictions, graph)\n",
    "                    results[eval_name] = scores\n",
    "                    print(f\"üìä Coer√™ncia Temporal: {scores.get('coherence_score', 'N/A')}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Grafo n√£o dispon√≠vel para avalia√ß√£o temporal\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao testar {eval_name}: {e}\")\n",
    "            results[eval_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Preparar dados de exemplo para avalia√ß√£o\n",
    "sample_predictions = [\n",
    "    \"Jesus ensinou aos disc√≠pulos sobre o amor e o perd√£o, mostrando o caminho da salva√ß√£o.\",\n",
    "    \"Os ap√≥stolos seguiram Jesus em sua jornada, aprendendo com seus ensinamentos e milagres.\",\n",
    "    \"A mensagem de esperan√ßa e f√© foi central na prega√ß√£o de Jesus aos povos.\"\n",
    "]\n",
    "\n",
    "sample_references = [\n",
    "    \"Jesus Cristo ensinou sobre o amor divino e o perd√£o, revelando o caminho para a vida eterna.\",\n",
    "    \"Os doze ap√≥stolos acompanharam Jesus durante seu minist√©rio, sendo testemunhas de seus milagres.\",\n",
    "    \"A proclama√ß√£o da boa nova trouxe esperan√ßa e f√© para todos os que ouviram a palavra de Deus.\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüìö Dados de exemplo para avalia√ß√£o:\")\n",
    "print(f\"Predi√ß√µes: {len(sample_predictions)} resumos\")\n",
    "print(f\"Refer√™ncias: {len(sample_references)} resumos\")\n",
    "\n",
    "# Executar testes de avalia√ß√£o\n",
    "if evaluators:\n",
    "    print(\"\\nüß™ Executando testes de avalia√ß√£o...\")\n",
    "    evaluation_results = test_evaluation_metrics(evaluators, sample_predictions, sample_references)\n",
    "    \n",
    "    print(f\"\\nüìà Resumo da Avalia√ß√£o:\")\n",
    "    for eval_name, result in evaluation_results.items():\n",
    "        status = \"‚úÖ Sucesso\" if result else \"‚ùå Falha\"\n",
    "        print(f\"{eval_name.upper()}: {status}\")\n",
    "        \n",
    "    # Calcular score m√©dio (se dispon√≠vel)\n",
    "    if evaluation_results.get('rouge') and evaluation_results.get('bertscore'):\n",
    "        rouge_l = evaluation_results['rouge'].get('rougeL', 0)\n",
    "        bert_f1 = np.mean(evaluation_results['bertscore'].get('f1', [0]))\n",
    "        avg_score = (rouge_l + bert_f1) / 2\n",
    "        print(f\"\\nüéØ Score m√©dio (ROUGE-L + BERTScore): {avg_score:.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum avaliador dispon√≠vel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final exploration report\n",
    "print(\"üìã TAEG DATA EXPLORATION FINAL REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Data status\n",
    "if 'data_loader' in locals() and data_loader:\n",
    "    print(f\"‚úÖ DataLoader: Configured and functional\")\n",
    "    if 'events' in locals() and events:\n",
    "        print(f\"‚úÖ Events loaded: {len(events)} events\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Events: Not loaded (XML files required)\")\n",
    "else:\n",
    "    print(f\"‚ùå DataLoader: Not configured\")\n",
    "\n",
    "# Graph status\n",
    "if 'graph' in locals() and graph:\n",
    "    print(f\"‚úÖ TAEG Graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
    "else:\n",
    "    print(f\"‚ùå TAEG Graph: Not built\")\n",
    "\n",
    "# Model status\n",
    "if 'taeg_model' in locals() and taeg_model:\n",
    "    total_params = sum(p.numel() for p in taeg_model.parameters())\n",
    "    print(f\"‚úÖ TAEG Model: Configured ({total_params:,} parameters)\")\n",
    "else:\n",
    "    print(f\"‚ùå TAEG Model: Not configured\")\n",
    "\n",
    "if 'baseline_models' in locals():\n",
    "    available_baselines = [name for name, model in baseline_models.items() if model is not None]\n",
    "    print(f\"‚úÖ Baseline Models: {', '.join(available_baselines) if available_baselines else 'None'}\")\n",
    "else:\n",
    "    print(f\"‚ùå Baseline Models: Not configured\")\n",
    "\n",
    "# Training status\n",
    "if 'training_config' in locals():\n",
    "    print(f\"‚úÖ Training Configuration: Defined\")\n",
    "    if 'train_data' in locals() and train_data:\n",
    "        print(f\"‚úÖ Training Data: {len(train_data)} samples\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Training Data: Not prepared\")\n",
    "else:\n",
    "    print(f\"‚ùå Training Configuration: Not defined\")\n",
    "\n",
    "# Evaluation status\n",
    "if 'evaluators' in locals():\n",
    "    available_evaluators = [name for name, eval in evaluators.items() if eval is not None]\n",
    "    print(f\"‚úÖ Evaluators: {', '.join(available_evaluators) if available_evaluators else 'None'}\")\n",
    "else:\n",
    "    print(f\"‚ùå Evaluators: Not configured\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ RECOMMENDED NEXT STEPS:\")\n",
    "print(\"1. Add gospel XML files to data/ directory\")\n",
    "print(\"2. Execute complete pipeline with real data\")\n",
    "print(\"3. Train TAEG model with gospel data\")\n",
    "print(\"4. Compare performance with baseline models\")\n",
    "print(\"5. Evaluate quality of generated summaries\")\n",
    "print(\"6. Detailed analysis of temporal coherence\")\n",
    "\n",
    "print(\"\\nüí° TIPS:\")\n",
    "print(\"- Use 'python main.py --help' to see command line options\")\n",
    "print(\"- Configure GPU if available for faster training\")\n",
    "print(\"- Adjust hyperparameters based on initial results\")\n",
    "print(\"- Monitor validation metrics to avoid overfitting\")\n",
    "\n",
    "print(\"\\nüî¨ This notebook provides a solid foundation for TAEG project exploration and development!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
